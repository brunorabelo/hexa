{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pylexique import Lexique383\n",
    "import os\n",
    "from pathlib import Path\n",
    "import data\n",
    "import pandas as pd\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "current_path = os.path.abspath('')\n",
    "BASE_PATH = Path(current_path)\n",
    "def get_absolute_file_path(path):\n",
    "    return os.path.join(BASE_PATH, path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_file_name = get_absolute_file_path('data/train.csv')\n",
    "train_data = pd.read_csv(train_data_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data.preprocess(train_data, create_keywords=True, clean_text=True, count_abreviations=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "LEXIQUE = Lexique383()\n",
    "lexique = set(LEXIQUE.lexique.keys())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "train_data[\"abrev\"] = train_data[\"text\"].apply(lambda s: data.count_abreviations_func(s, lexique))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# select useful columns\n",
    "# train_data_filtered = train_data.drop([\"text\", \"urls\", \"mentions\", \"hashtags\", \"timestamp\", \"TweetID\"], axis=1)\n",
    "train_data_filtered = train_data.loc[:, [\"retweets_count\",\"favorites_count\",\"followers_count\",\"statuses_count\",\"friends_count\",\n",
    "                                 \"hashtags_count\",\"hour\",\"verified\",\"url_count\",\"text_len\",\"rt\",\"Macron\",\"Zemmour\",\"Melenchon\"]]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "X_train, X_eval, y_train, y_eval = train_test_split(train_data_filtered.drop(\"retweets_count\", axis=1),\n",
    "                                                    train_data_filtered[\"retweets_count\"],\n",
    "                                                    random_state=42, test_size=0.2)\n",
    "\n",
    "# Standardize the data\n",
    "normal_columns = train_data_filtered.drop([\"hour\", \"verified\", \"Macron\", \"Zemmour\", \"Melenchon\", \"url_count\", \"rt\", \"retweets_count\"], axis=1).columns\n",
    "mu, sigma = X_train[normal_columns].mean(axis=0), X_train[normal_columns].std(axis=0)\n",
    "X_train.loc[:, normal_columns] = (X_train[normal_columns] - mu) / sigma\n",
    "X_eval.loc[:, normal_columns] = (X_eval[normal_columns] - mu) / sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,\n             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n             early_stopping_rounds=None, enable_categorical=False,\n             eval_metric=None, feature_types=None, gamma=0, gpu_id=-1,\n             grow_policy='depthwise', importance_type=None,\n             interaction_constraints='', learning_rate=0.300000012, max_bin=256,\n             max_cat_threshold=64, max_cat_to_onehot=4, max_delta_step=0,\n             max_depth=6, max_leaves=0, min_child_weight=1, missing=nan,\n             monotone_constraints='()', n_estimators=100, n_jobs=0,\n             num_parallel_tree=1, predictor='auto', random_state=42, ...)"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "xgb_model.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.1741439594374885\n"
     ]
    }
   ],
   "source": [
    "y_pred = xgb_model.predict(X_eval)\n",
    "mae = mean_absolute_error(y_eval, y_pred)\n",
    "print(mae)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "869fcbf3262855e1eaf5654d341d7f712e3ffc1dffd54e403b0f2d04ee7ae421"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
