{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"data/train_with_embeddings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new features\n",
    "train_data[\"url_count\"] = train_data[\"urls\"].apply(lambda s: s[1:-1].count(\"\\'\")/2)\n",
    "train_data[\"text_len\"] = train_data[\"text\"].apply(lambda s: len(s))\n",
    "train_data[\"hashtags_count\"] = train_data[\"hashtags\"].apply(lambda s: s[1:-1].count(\"\\'\")/2)\n",
    "train_data[\"day\"] = train_data[\"timestamp\"].apply(lambda t: datetime.utcfromtimestamp(t/1000).day)\n",
    "train_data[\"hour\"] = train_data[\"timestamp\"].apply(lambda t: datetime.utcfromtimestamp(t/1000).hour)\n",
    "\n",
    "# text features\n",
    "train_data[\"avg_word_len\"] = train_data[\"text\"].apply(lambda s: np.mean([len(w) for w in s.split()]))\n",
    "train_data[\"rep_words_freq\"] = train_data[\"text\"].apply(lambda s: np.mean(len(list(set(s.split())))/len(s.split())))\n",
    "train_data[\"rep_chars_freq\"] = train_data[\"text\"].apply(lambda s: np.mean(len(list(set(s)))/len(list(s))))\n",
    "train_data[\"max_char_freq\"] = train_data[\"text\"].apply(lambda s: max( [s.count(c) for c in list(set(s))] )   /len(list(s)))\n",
    "train_data[\"avg_word_count\"] = train_data[\"text\"].apply(lambda s: len(s.split()))\n",
    "\n",
    "# indicators of keywords\n",
    "train_data[\"Macron\"] =  train_data[\"text\"].apply(lambda s: (\"macron\" in s.lower().split()))\n",
    "train_data[\"Zemmour\"] =  train_data[\"text\"].apply(lambda s: (\"zemmour\" in s.lower().split()))\n",
    "train_data[\"Melenchon\"] =  train_data[\"text\"].apply(lambda s: (\"melenchon\" in s.replace(\"Ã©\",\"e\").lower().split()))\n",
    "train_data[\"rt\"] =  train_data[\"text\"].apply(lambda s: (\"rt\" in s.lower().split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select useful columns\n",
    "train_data_filtered = train_data.drop([\"text\", \"urls\", \"mentions\", \"hashtags\", \"timestamp\", \"TweetID\"], axis=1)\n",
    "# train_data_filtered = train_data.loc[:, [\"retweets_count\",\"favorites_count\",\"followers_count\",\"statuses_count\",\"friends_count\",\n",
    "#                                  \"hashtags_count\",\"hour\",\"verified\",\"url_count\",\"text_len\",\"rt\",\"Macron\",\"Zemmour\",\"Melenchon\"]]\n",
    "\n",
    "\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(train_data_filtered.drop(\"retweets_count\", axis=1),\n",
    "                                                    train_data_filtered[\"retweets_count\"],\n",
    "                                                    random_state=42, test_size=0.1)\n",
    "\n",
    "# Standardize the data\n",
    "normal_columns = train_data_filtered.drop([\"hour\", \"verified\", \"Macron\", \"Zemmour\", \"Melenchon\", \"url_count\", \"rt\", \"retweets_count\"], axis=1).columns\n",
    "mu, sigma = X_train[normal_columns].mean(axis=0), X_train[normal_columns].std(axis=0)\n",
    "X_train.loc[:, normal_columns] = (X_train[normal_columns] - mu) / sigma\n",
    "X_eval.loc[:, normal_columns] = (X_eval[normal_columns] - mu) / sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)[..., np.newaxis]\n",
    "X_eval = np.array(X_eval)[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(8, 3, activation='relu', padding='same',\n",
    "                            input_shape=(X_train.shape[1], 1)))\n",
    "model.add(Conv1D(8, 3, activation='relu', padding='same',\n",
    "                            input_shape=(X_train.shape[1], 1)))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow_addons as tfa\n",
    "# optimizer = tfa.optimizers.AdamW(learning_rate=1e-3, weight_decay=1e-4)\n",
    "# model.compile(optimizer=optimizer, loss='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train.astype(np.float32), y_train.astype(np.float32), epochs=200, batch_size=1024,\n",
    "         validation_data=(X_eval.astype(np.float32), y_eval.astype(np.float32)), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.epoch, history.history[\"loss\"], 'b', label='Training loss')\n",
    "plt.plot(history.epoch, history.history[\"val_loss\"], 'g', label='Validation loss')\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_eval.values.astype(np.float32),  y_eval.values.astype(np.float32), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_train.values.astype(np.float32))\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(X_train[\"favorites_count\"], model.predict(X_train))\n",
    "# plt.scatter(X_train[\"favorites_count\"], y_train.values.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "\n",
    "eval_data = pd.read_csv(\"data/evaluation_with_embeddings.csv\")\n",
    "tweets = eval_data[\"TweetID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data[\"url_count\"] = eval_data[\"urls\"].apply(lambda s: s[1:-1].count(\"\\'\")/2)\n",
    "eval_data[\"text_len\"] = eval_data[\"text\"].apply(lambda s: len(s))\n",
    "eval_data[\"hashtags_count\"] = eval_data[\"hashtags\"].apply(lambda s: s[1:-1].count(\"\\'\")/2)\n",
    "eval_data[\"day\"] = eval_data[\"timestamp\"].apply(lambda t: datetime.utcfromtimestamp(t/1000).day)\n",
    "eval_data[\"hour\"] = eval_data[\"timestamp\"].apply(lambda t: datetime.utcfromtimestamp(t/1000).hour)\n",
    "eval_data[\"Macron\"] =  eval_data[\"text\"].apply(lambda s: (\"macron\" in s.lower().split()))\n",
    "eval_data[\"Zemmour\"] =  eval_data[\"text\"].apply(lambda s: (\"zemmour\" in s.lower().split()))\n",
    "eval_data[\"Melenchon\"] =  eval_data[\"text\"].apply(lambda s: (\"melenchon\" in s.lower().split()))\n",
    "eval_data[\"rt\"] =  eval_data[\"text\"].apply(lambda s: (\"rt\" in s.lower().split()))\n",
    "\n",
    "# print(\"sentiment analysis...\")\n",
    "# eval_data[\"compound\"] =  eval_data[\"text\"].apply(lambda s: sia.polarity_scores(s)['compound'])\n",
    "\n",
    "eval_data = eval_data.drop([\"text\", \"urls\", \"mentions\", \"hashtags\", \"timestamp\", \"TweetID\"], axis=1)\n",
    "\n",
    "# normalize\n",
    "eval_data.loc[:, normal_columns] = (eval_data.loc[:, normal_columns] - mu) / sigma\n",
    "\n",
    "print(eval_data)\n",
    "\n",
    "pred = model.predict(eval_data.values.astype(np.float32))\n",
    "\n",
    "print(pred)\n",
    "\n",
    "# output normalization\n",
    "for i,p in enumerate(pred):\n",
    "    if p<0: pred[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/predictions.csv\", 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"TweetID\", \"retweets_count\"])\n",
    "    for index, prediction in enumerate(pred):\n",
    "        writer.writerow([str(tweets[index]) , str(int(prediction))])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2b76f9357de510751682414c7cddbaacea429d985ca72e90da955bd41bf6fe1e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
